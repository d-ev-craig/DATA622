{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6147bda8-2d36-466c-a973-bec3bf8b4dce",
   "metadata": {},
   "source": [
    "LDA in Python Walkthrough:\n",
    "\n",
    "https://hands-on.cloud/implementation-of-linear-discriminant-analysis-lda-using-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3529c9f-fa32-442a-8187-78f967c8fb35",
   "metadata": {},
   "source": [
    "# Discriminant Analysis Notes\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Linear Discriminant Analysis is used for both classification and dimensionality reduction. In classification, LDA finds linear combinations of features that best separate multiple classes or groups and maximizes between-class varaince, while minimizing within-class variance. \n",
    "\n",
    "You can compare this to PCA, but instead of focusing on data points that contain the most variation, we want to maximize the separability among the known categories. So both LDA and PCA are dimensionality reduction techniques, but for different purposes.\n",
    "\n",
    "### Why Discriminant Analysis:\n",
    "\n",
    "When classes are well-separated, logistic regressions are unstable (seeing as how they force themselves to 0 or 1). Linear discriminant analysis does not suffer from this problem.\n",
    "\n",
    "Linear Discriminant analysis is useful when you have more than two response classes, because it also provides a low-dimensional view of the data\n",
    "\n",
    "### How Discriminant Analysis:\n",
    "\n",
    "Imagine having an x variable and a y variable that both impact the success of a certain drug. LDA attempts to draw a new axis cutting through the x and y observations that maximizes the distance between success observations and failure observations.\n",
    "\n",
    "It does this by two main criteria:\n",
    "\n",
    "1. The new axis maximizies the distance between the means of fail and success classes. By maximizing the distance, it makes it easier to classify whether an observation was success or failure since they're so far apart. Let's say we have $\\mu_1$ for success and $\\mu_2$ for failure that represent the means of each class.\n",
    "\n",
    "2. The new axis minimizes the variation between observations within a class. You can think of this has having a tight grouping on a shooting target. Let's say we have $s^2_1$ and $s^2_2$ that represent the variation of success observations and failure observations respectively.\n",
    "\n",
    "3. You can now create a ratio $$\\frac{(\\mu_1 - \\mu_2)^2}{s^2_1 + s^2_2}$$\n",
    "    - we square the numerator to ensure the value stays positive\n",
    "    - ideally we want a large numerator and a small denominator\n",
    "    - by creating a ratio between the two criteria, we can accomodate scenarios where values along a variable are not that different between classes, but are very different for another variable.\n",
    "\n",
    "This can then be simplified to below where d represents \"distance\" between the means.\n",
    "\n",
    "$$\\frac{(d)^2}{s^2_1 + s^2_2}$$\n",
    "\n",
    "\n",
    "Summary from another lecture:\n",
    "The approach in Discriminant Analysis is to model the distribution of X in each of its classes separately (supposing that X had several classes). From there, one uses Bayes theorem to flip things around and obtain Pr(Y|X).\n",
    "\n",
    "This is different than normal distributions (Gaussians) where distributions for each class leads to linear or quadratic discriminant analysis. This Guassian approach is quite general, many others can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425744cf-c55a-4ef0-bfe1-db2d4991d41a",
   "metadata": {},
   "source": [
    "### Handling 3+ Variables\n",
    "\n",
    "It's almost the exact same process. One creates an axis that maximizes the difference between the two means of the classes. Remember that this axis is effectively just a linear combination of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a027d-d0f4-4b83-9613-91367e63d292",
   "metadata": {},
   "source": [
    "### Handling 3+ Categories\n",
    "\n",
    "Two things change, but just barely.\n",
    "![My Image](LDA for 3 Categories.png)\n",
    "\n",
    "\n",
    "1. We change how distance is measured amongst the means. We identify a point that is central to all data. Then distance is maximized between each category of points, while minimizing for scatter.\n",
    "2. We create two axes to separate the data. This is done by creating 3 points, one for each category that is central to each class and use those points to optimize separation by maximizing distance between two axes created to seperate the categories.\n",
    "\n",
    "This two axes method is powerful since it can handle any number of variables since we are only maximizing distance beteween points and the two axes.\n",
    "\n",
    "\n",
    "### Compare/Contrast between PCA and LDA\n",
    "\n",
    "- Both rank the new axes in order of importance\n",
    "    - PC1 from PCA, accounts for most variation in data and so on with PC2, PC3..\n",
    "    - LD1 accounts for most variation between categories, and so on with LD2   \n",
    "- Both methods allow you to see which genes are driving the new axes\n",
    "- Both try to reduce dimensionality\n",
    "    - PCA looks at genes with most variation\n",
    "    - LDA maximizes distance between categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c18fa-d4b4-4c59-8810-1b1e788b77e7",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Bayes Theorem:\n",
    "$Pr(Y = k| X = x) = \n",
    "\\frac{Pr(X = x| Y = k) * Pr(Y = k)}{ Pr(X = x)}$\n",
    "\n",
    "In writing, one would describe the above as swapping the probability around. Instead of having the probability of Y = k given X = x, Bayes theorem allows us to flip it and multiply our flipped probability by the probability of the original variable of interest. AKA The probability of Y = k given X = x, is equal to the product of the probability of X = x, given Y = k multiplied by the probability that Y = k divided by the probability of X = x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0744f-5827-46af-a00f-bb796574e3d8",
   "metadata": {},
   "source": [
    "## Bayes Theorem in Discriminant Analysis\n",
    "\n",
    "When writing Bayes Theorem in Discriminant Analysis, the writing changes to below. Note that $pi$ is used in lieu of a greek variable below and does not represent its numerical value.\n",
    "\n",
    "$Pr(Y=k | X=x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{k}\\pi_l f_l(x)}$\n",
    "\n",
    "* $f_k(x) = Pr(X=x|Y=k)$ is the **density** for X in class *k*. \n",
    "    - Remember that \"density\" of X is effectively the area under the curve for a function(x).\n",
    "    \n",
    "* $pi_k = Pr(Y=k)$ is the marginal or **prior** probability for class *k* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
